{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T20:02:22.120033Z",
     "start_time": "2025-07-23T20:02:11.572771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statistics\n",
    "import torch.nn.functional as F\n",
    "from exceptiongroup import catch\n",
    "from tqdm import tqdm\n",
    "from itertools import count\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, Overcooked\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "from collections import namedtuple\n",
    "import pygame\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "from overcooked_ai_py.mdp.actions import Action"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T20:02:22.384936Z",
     "start_time": "2025-07-23T20:02:22.370938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OvercookedRewardShaping(Overcooked): # Using OvercookedGridworld if it's the base\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.prev_agent_obs = [None, None] # To store observations from previous step for both agents\n",
    "\n",
    "    def step(self, actions):\n",
    "        observation, base_reward, done, info = super().step(actions)\n",
    "\n",
    "        # Calculate shaped reward for each agent\n",
    "        shaped_reward_total = 0\n",
    "        current_agent_obs = observation['both_agent_obs']\n",
    "\n",
    "        # Ensure prev_agent_obs is initialized for the first step\n",
    "        if self.prev_agent_obs[0] is None:\n",
    "            self.prev_agent_obs = current_agent_obs\n",
    "\n",
    "        for i, obs in enumerate(current_agent_obs):\n",
    "            # Pass current and previous observation for this agent\n",
    "            shaped_reward_total += self._compute_agent_shaping(obs, self.prev_agent_obs[i])\n",
    "\n",
    "        # Update previous observations for the next step\n",
    "        self.prev_agent_obs = current_agent_obs\n",
    "\n",
    "        final_shaped_reward = base_reward + shaped_reward_total\n",
    "\n",
    "        #Optional: Print base reward only if it's non-zero for clarity\n",
    "        if base_reward != 0:\n",
    "             print(f\"Soup delivered!\")\n",
    "\n",
    "        return observation, base_reward,final_shaped_reward, done, info\n",
    "\n",
    "    def _compute_agent_shaping(self, current_obs, prev_obs):\n",
    "        shaping = 0.0\n",
    "\n",
    "        ONION_IDX = 0\n",
    "        SOUP_IDX = 1\n",
    "        DISH_IDX = 2\n",
    "        TOMATO_IDX = 3\n",
    "\n",
    "\n",
    "        POT_EMPTY_IDX = 0\n",
    "        POT_FULL_IDX = 1\n",
    "        POT_COOKING_IDX = 2\n",
    "        POT_READY_IDX = 3\n",
    "\n",
    "\n",
    "        prev_holding_vector = prev_obs[4:8]\n",
    "        current_holding_vector = current_obs[4:8]\n",
    "\n",
    "        # Was not holding anything, now holding an ingredient or empty dish\n",
    "        if prev_holding_vector.sum() == 0 and current_holding_vector.sum() == 1:\n",
    "            if current_holding_vector[ONION_IDX] == 1 or current_holding_vector[TOMATO_IDX] == 1:\n",
    "                shaping += 0.05 # Reward for picking up an ingredient\n",
    "            elif current_holding_vector[DISH_IDX] == 1:\n",
    "                shaping += 0.02 # Reward for picking up an empty dish\n",
    "\n",
    "        prev_pot_onions = prev_obs[27:28][0] # Assuming single value\n",
    "        current_pot_onions = current_obs[27:28][0]\n",
    "        prev_pot_tomatoes = prev_obs[28:29][0]\n",
    "        current_pot_tomatoes = current_obs[28:29][0]\n",
    "\n",
    "        # Check if ingredient count in closest pot increased\n",
    "        if current_pot_onions > prev_pot_onions:\n",
    "            shaping += 0.1 # Reward for adding onion\n",
    "        if current_pot_tomatoes > prev_pot_tomatoes:\n",
    "            shaping += 0.1 # Reward for adding tomato\n",
    "\n",
    "\n",
    "\n",
    "        prev_pot_states = prev_obs[23:27]\n",
    "        current_pot_states = current_obs[23:27]\n",
    "\n",
    "        # Transition from full to cooking\n",
    "        if prev_pot_states[POT_FULL_IDX] == 1 and current_pot_states[POT_COOKING_IDX] == 1:\n",
    "            shaping += 0.2 # Reward for starting to cook\n",
    "\n",
    "        # Transition from cooking to ready\n",
    "        if prev_pot_states[POT_COOKING_IDX] == 1 and current_pot_states[POT_READY_IDX] == 1:\n",
    "            shaping += 0.3 # Reward for soup becoming ready\n",
    "\n",
    "\n",
    "        if prev_pot_states[POT_READY_IDX] == 1 and current_holding_vector[SOUP_IDX] == 1 and prev_holding_vector[SOUP_IDX] == 0:\n",
    "            shaping += 0.25 # Reward for picking up a ready soup\n",
    "\n",
    "\n",
    "        current_dx_serving = abs(current_obs[16:17][0]) # abs(dx)\n",
    "        current_dy_serving = abs(current_obs[17:18][0]) # abs(dy)\n",
    "        prev_dx_serving = abs(prev_obs[16:17][0])\n",
    "        prev_dy_serving = abs(prev_obs[17:18][0])\n",
    "\n",
    "        current_dist_serving = current_dx_serving + current_dy_serving # Manhattan distance\n",
    "        prev_dist_serving = prev_dx_serving + prev_dy_serving\n",
    "\n",
    "        if current_holding_vector[SOUP_IDX] == 1 and current_dist_serving < prev_dist_serving:\n",
    "            shaping += 0.01 # Small continuous reward for moving towards serving\n",
    "\n",
    "\n",
    "\n",
    "        return shaping"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T20:02:22.508146Z",
     "start_time": "2025-07-23T20:02:22.401978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "class OvercookedDQN:\n",
    "    def __init__(\n",
    "            self,\n",
    "            layout_name,\n",
    "            model_DQN,\n",
    "            gamma,  # Discount factor\n",
    "            lr_model,\n",
    "            epochs,  # Number of optimization epochs\n",
    "            batch_size,\n",
    "            optimizer_class,\n",
    "            epsilon_decay,\n",
    "            epsilon_start,\n",
    "            epsilon_end,\n",
    "            TAU):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lr_model = lr_model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.TAU = TAU\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        base_mdp = OvercookedGridworld.from_layout_name(layout_name)  # or other layout\n",
    "        base_env = OvercookedEnv.from_mdp(base_mdp, info_level=0, horizon=400)\n",
    "        #self.env = env = Overcooked(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "        self.env  = OvercookedRewardShaping(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "\n",
    "        self.n_possible_action = self.env.action_space.n\n",
    "\n",
    "        dummy_state = self.env.reset()\n",
    "        dummy_obs_agent0 = dummy_state['both_agent_obs'][0]\n",
    "        state_input_size = len(dummy_obs_agent0)  # Assuming the featurized state is a flat vector\n",
    "\n",
    "        self.policy_net = model_DQN(state_input_size, self.n_possible_action).to(self.device)\n",
    "        self.target_net = DQN(state_input_size, self.n_possible_action).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.model_optimizer = optimizer_class(self.policy_net.parameters(), lr=lr_model)\n",
    "\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "\n",
    "    def select_action(self,state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=self.device, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def optimize_model(self,agent):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        agent_states = [s[agent] for s in batch.state]\n",
    "        agent_actions = [a[agent] for a in batch.action]\n",
    "        agent_next_states = [ns[agent] for ns in batch.next_state]\n",
    "\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              agent_next_states)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in agent_next_states\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(agent_states)\n",
    "        action_batch = torch.cat(agent_actions)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.model_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.model_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    def trainingLoop(self, max_episodes):\n",
    "        for i_episode in range(max_episodes):\n",
    "            rewards = 0\n",
    "            # Initialize the environment and get its state\n",
    "            state = self.env.reset()\n",
    "            state0 = torch.tensor(state['both_agent_obs'][0], dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            state1 = torch.tensor(state['both_agent_obs'][1], dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            for t in count():\n",
    "                action0 = self.select_action(state0)\n",
    "                action1 = self.select_action(state1)\n",
    "                obs, reward, shaped_reward, done, info = self.env.step((action0.item(),action1.item()))\n",
    "                done = done\n",
    "\n",
    "                reward += shaped_reward\n",
    "\n",
    "                if (state['both_agent_obs'][0] == obs['both_agent_obs'][0]).all() or (state['both_agent_obs'][1] == obs['both_agent_obs'][1]).all():\n",
    "                    reward += -0.1 * t\n",
    "\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "                if done:\n",
    "                    next_state0 = None\n",
    "                    next_state1 = None\n",
    "                else:\n",
    "                    next_state0 = torch.tensor(obs['both_agent_obs'][0], dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                    next_state1 = torch.tensor(obs['both_agent_obs'][1], dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "                # Store the transition in memory\n",
    "                rewards = rewards + reward.item()\n",
    "                self.memory.push((state0, state1), (action0,action1), (next_state0,next_state1), reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state0 = next_state0\n",
    "                state1 = next_state1\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                loss = self.optimize_model(0)\n",
    "                loss = self.optimize_model(1)\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    if i_episode % 10 == 0:\n",
    "                        print(f\"Episode: {i_episode + 1}, DQN loss : {loss}, total reward {rewards}\")\n",
    "\n",
    "\n",
    "                    break\n",
    "\n",
    "        print('Complete')\n",
    "\n",
    "    def testVisualize(self,print_action=False):\n",
    "        pygame.init()\n",
    "        visualizer = StateVisualizer()\n",
    "\n",
    "        # 2) Grab your grid and do one dummy render to get a surface\n",
    "        grid = self.env.base_env.mdp.terrain_mtx\n",
    "        _ = self.env.reset()\n",
    "        surf = visualizer.render_state(self.env.base_env.state, grid=grid)\n",
    "\n",
    "        # 3) Use that surface’s size for your window\n",
    "        win_w, win_h = surf.get_size()\n",
    "        screen = pygame.display.set_mode((win_w, win_h), pygame.RESIZABLE)\n",
    "        clock = pygame.time.Clock()\n",
    "\n",
    "        running = True\n",
    "        obs = self.env.reset()  #observation of the starting state\n",
    "        soup_delivered = 0\n",
    "\n",
    "        total_rewards = []\n",
    "        while running:\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            self.policy_net.eval()  # Set model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                if print_action:\n",
    "                    print(f\"osservazione iniziale: {obs['both_agent_obs'][0]}\")\n",
    "                state0_tensor = torch.tensor(obs['both_agent_obs'][0], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                state1_tensor = torch.tensor(obs['both_agent_obs'][1], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "                logits0 = self.policy_net(state0_tensor)\n",
    "                logits1 = self.policy_net(state1_tensor)\n",
    "\n",
    "                action0 = torch.argmax(logits0, dim=1).item()\n",
    "                action1 = torch.argmax(logits1, dim=1).item()\n",
    "\n",
    "                if print_action:\n",
    "                    print(action0, action1)\n",
    "                # try to step; if episode is over, catch and reset\n",
    "                try:\n",
    "                    # Overcooked wrapper returns (obs_p0, obs_p1, reward, done, info)\n",
    "                    obs, reward,shaped_reward, done, info = self.env.step((action0, action1))\n",
    "\n",
    "                    if(reward):\n",
    "                        soup_delivered += 1\n",
    "\n",
    "\n",
    "                except AssertionError:\n",
    "                    # base_env.is_done() was True → reset and continue\n",
    "                    self.env.reset()\n",
    "                    break\n",
    "\n",
    "                # render the new state\n",
    "                surf = visualizer.render_state(self.env.base_env.state, grid=grid)\n",
    "\n",
    "                # draw it\n",
    "                screen.blit(surf, (0, 0))\n",
    "                pygame.display.flip()\n",
    "\n",
    "                clock.tick(15)  # cap at 30 FPS\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "        print(f\"Soup delivered: {soup_delivered}\")\n",
    "\n",
    "\n",
    "# Test the trained agent\n",
    "def test_agent(agente, num_episodes=1000):\n",
    "    \"\"\"Test agent performance without learning or exploration.\"\"\"\n",
    "    total_rewards = []\n",
    "    average_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        obs = agente.env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        i = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            state0_tensor = torch.tensor(obs['both_agent_obs'][0], dtype=torch.float32).unsqueeze(0).to(agente.device)\n",
    "            state1_tensor = torch.tensor(obs['both_agent_obs'][1], dtype=torch.float32).unsqueeze(0).to(agente.device)\n",
    "\n",
    "            logits0 = agente.policy_net(state0_tensor)\n",
    "            logits1 = agente.policy_net(state1_tensor)\n",
    "\n",
    "            action0 = torch.argmax(logits0, dim=1).item()\n",
    "            action1 = torch.argmax(logits1, dim=1).item()\n",
    "\n",
    "            obs, reward,shaped_reward, done, info = agente.env.step((action0, action1))\n",
    "\n",
    "            episode_reward += reward\n",
    "            i = i+1\n",
    "\n",
    "        average_rewards.append(episode_reward/i)\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"average reward in episode {episode} : {num_episodes}\")\n",
    "\n",
    "\n",
    "    average_reward = np.mean(total_rewards)\n",
    "\n",
    "    print(f\"Test Results over {num_episodes} episodes:\")\n",
    "    print(f\"Average Reward: {average_reward:.3f}\")\n",
    "    print(f\"Standard Deviation: {np.std(total_rewards):.3f}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T20:07:18.068171Z",
     "start_time": "2025-07-23T20:02:22.526147Z"
    }
   },
   "cell_type": "code",
   "source": "DQNO = OvercookedDQN(\"corridor\",model_DQN =DQN,gamma = 0.99,lr_model = 0.001,epochs = 10,batch_size = 128,optimizer_class = torch.optim.Adam,TAU=0.005,epsilon_decay=50000,epsilon_start=0.9,epsilon_end=0.05)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-23T20:07:24.365580Z"
    }
   },
   "cell_type": "code",
   "source": "DQNO.trainingLoop(max_episodes=500)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, DQN loss : 4.31758671766147e-05, total reward -0.800000011920929\n",
      "Episode: 11, DQN loss : 0.002235371619462967, total reward 0.0\n",
      "Episode: 21, DQN loss : 0.001648878213018179, total reward 0.12999999895691872\n",
      "Episode: 31, DQN loss : 0.004468138329684734, total reward -6.1199999041855335\n",
      "Episode: 41, DQN loss : 0.006144186016172171, total reward 0.07999999821186066\n",
      "Episode: 51, DQN loss : 0.018222419545054436, total reward 0.1599999964237213\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:51:21.727373Z",
     "start_time": "2025-07-23T19:51:15.136608Z"
    }
   },
   "cell_type": "code",
   "source": "test_agent(DQNO,num_episodes=10)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:05,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 0 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:05,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 1 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:04,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 2 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:02<00:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 3 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:03<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 4 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:03<00:02,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 5 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:04<00:01,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 6 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:05<00:01,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 7 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:05<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 8 : 10\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "Soup delivered! Base Reward: 20. Shaped Reward Total: 0.0. Final: 20.0\n",
      "average reward in episode 9 : 10\n",
      "Test Results over 10 episodes:\n",
      "Average Reward: 180.000\n",
      "Standard Deviation: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
