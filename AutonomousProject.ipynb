{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-19T07:48:29.054695Z",
     "start_time": "2025-07-19T07:48:25.848215Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, Overcooked\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T07:48:29.086695Z",
     "start_time": "2025-07-19T07:48:29.070697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Overcooked_Qlearn:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layout_name: str,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        n_episodes,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Q-Learning agent.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            learning_rate: How quickly to update Q-values (0-1)\n",
    "            initial_epsilon: Starting exploration rate (usually 1.0)\n",
    "            epsilon_decay: How much to reduce epsilon each episode\n",
    "            final_epsilon: Minimum exploration rate (usually 0.1)\n",
    "            discount_factor: How much to value future rewards (0-1)\n",
    "        \"\"\"\n",
    "        base_mdp = OvercookedGridworld.from_layout_name(layout_name) # or other layout\n",
    "        base_env = OvercookedEnv.from_mdp(base_mdp, info_level=0, horizon=400)\n",
    "        env = Overcooked(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.possible_action = [*itertools.product(range(6),repeat=2)]\n",
    "\n",
    "\n",
    "        # Q-table: maps (state, action) to expected reward\n",
    "        # defaultdict automatically creates entries with zeros for new states\n",
    "        self.q_values = defaultdict(lambda: np.zeros(len(self.possible_action)))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor  # How much we care about future rewards\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs):\n",
    "\n",
    "        # With probability epsilon: explore (random action)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample(),self.env.action_space.sample()\n",
    "\n",
    "        # With probability (1-epsilon): exploit (best known action)\n",
    "        else:\n",
    "            return self.possible_action[np.argmax(self.q_values[obs])]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs,\n",
    "        action,\n",
    "        reward,\n",
    "        terminated,\n",
    "        next_obs,\n",
    "    ):\n",
    "        \"\"\"Update Q-value based on experience.\n",
    "\n",
    "        This is the heart of Q-learning: learn from (state, action, reward, next_state)\n",
    "        \"\"\"\n",
    "        # What's the best we could do from the next state?\n",
    "        # (Zero if episode terminated - no future rewards possible)\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "\n",
    "        # What should the Q-value be? (Bellman equation)\n",
    "        target = reward + self.discount_factor * future_q_value\n",
    "\n",
    "        # How wrong was our current estimate?\n",
    "        temporal_difference = target - self.q_values[obs][self.possible_action.index(action)]\n",
    "\n",
    "        # Update our estimate in the direction of the error\n",
    "        # Learning rate controls how big steps we take\n",
    "        self.q_values[obs][self.possible_action.index(action)] = (\n",
    "            self.q_values[obs][self.possible_action.index(action)] + self.lr * temporal_difference\n",
    "        )\n",
    "\n",
    "        # Track learning progress (useful for debugging)\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ],
   "id": "9356720f48fadce6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T07:48:29.504218Z",
     "start_time": "2025-07-19T07:48:29.458696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.02        # How fast to learn (higher = faster but less stable)\n",
    "n_episodes = 5000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes *4)  # Reduce exploration over time\n",
    "final_epsilon = 0.2         # Always keep some exploration\n",
    "\n",
    "\n",
    "agent = Overcooked_Qlearn(\n",
    "    layout_name=\"cramped_room\",\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    n_episodes=n_episodes,\n",
    ")"
   ],
   "id": "9e9d9733a92998a6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T15:05:16.376490Z",
     "start_time": "2025-07-19T15:04:27.106022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "  # Progress bar\n",
    "n_episodes = 100\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs = agent.env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Play one complete hand\n",
    "    while not done:\n",
    "        # Agent chooses action (initially random, gradually more intelligent)\n",
    "        action = agent.get_action(tuple(obs['both_agent_obs'][0] + obs['both_agent_obs'][1]))\n",
    "        StateVisualizer().display_rendered_state(obs['overcooked_state'],window_display=True,grid=agent.env.mdp.terrain_mtx)\n",
    "\n",
    "\n",
    "        # Take action and observe result\n",
    "        next_obs, reward, terminated, event_info = agent.env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            subgoal_reward = event_info['episode'][\"ep_shaped_r\"]\n",
    "            reward = reward + (subgoal_reward - int(subgoal_reward/ ((episode + 10)*10)))\n",
    "\n",
    "\n",
    "        # Learn from this experience\n",
    "        agent.update(tuple(obs['both_agent_obs'][0] + obs['both_agent_obs'][1]),\n",
    "                     action, reward, terminated,\n",
    "                     tuple(next_obs['both_agent_obs'][0] + next_obs['both_agent_obs'][1]))\n",
    "\n",
    "\n",
    "        #StateVisualizer().display_rendered_state(next_obs['overcooked_state'],window_display=True,grid=agent.env.mdp.terrain_mtx)\n",
    "\n",
    "        # Move to next state\n",
    "        done = terminated\n",
    "        obs = next_obs\n",
    "    # Reduce exploration rate (agent becomes less random over time)\n",
    "    agent.decay_epsilon()"
   ],
   "id": "a63e87f97b77b34c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Agent chooses action (initially random, gradually more intelligent)\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mget_action(\u001B[38;5;28mtuple\u001B[39m(obs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth_agent_obs\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m obs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth_agent_obs\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m---> 11\u001B[0m     \u001B[43mStateVisualizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay_rendered_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43movercooked_state\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mwindow_display\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmdp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mterrain_mtx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# Take action and observe result\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     next_obs, reward, terminated, event_info \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Autonomous\\overcooked_ai\\src\\overcooked_ai_py\\visualization\\state_visualizer.py:258\u001B[0m, in \u001B[0;36mStateVisualizer.display_rendered_state\u001B[1;34m(self, state, hud_data, action_probs, grid, img_path, ipython_display, window_display)\u001B[0m\n\u001B[0;32m    255\u001B[0m     show_image_in_ipython(img_path)\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m window_display:\n\u001B[1;32m--> 258\u001B[0m     \u001B[43mrun_static_resizeable_window\u001B[49m\u001B[43m(\u001B[49m\u001B[43msurface\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow_fps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img_path\n",
      "File \u001B[1;32m~\\PycharmProjects\\Autonomous\\overcooked_ai\\src\\overcooked_ai_py\\visualization\\pygame_utils.py:21\u001B[0m, in \u001B[0;36mrun_static_resizeable_window\u001B[1;34m(surface, fps)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     20\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[1;32m---> 21\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[43mpygame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m event\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m QUIT:\n\u001B[0;32m     23\u001B[0m         pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mquit()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T15:04:11.220747Z",
     "start_time": "2025-07-19T15:03:42.490718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the trained agent\n",
    "def test_agent(agent, env, num_episodes=1000):\n",
    "    \"\"\"Test agent performance without learning or exploration.\"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    # Temporarily disable exploration for testing\n",
    "    old_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0  # Pure exploitation\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            StateVisualizer().display_rendered_state(obs['overcooked_state'],window_display=True,grid=agent.env.mdp.terrain_mtx)\n",
    "            action = agent.get_action(tuple(obs['both_agent_obs'][0] + obs['both_agent_obs'][1]))\n",
    "            next_obs, reward, terminated, event_info = env.step(action)\n",
    "            if reward > 0:\n",
    "                print(reward)\n",
    "                print(\"soup delivered\")\n",
    "            episode_reward += reward\n",
    "            done = terminated\n",
    "            obs = next_obs\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = old_epsilon\n",
    "\n",
    "    win_rate = np.mean(np.array(total_rewards) > 0)\n",
    "    average_reward = np.mean(total_rewards)\n",
    "\n",
    "    print(f\"Test Results over {num_episodes} episodes:\")\n",
    "    print(f\"Win Rate: {win_rate:.1%}\")\n",
    "    print(f\"Average Reward: {average_reward:.3f}\")\n",
    "    print(f\"Standard Deviation: {np.std(total_rewards):.3f}\")\n",
    "\n",
    "# Test your agent\n",
    "test_agent(agent, agent.env)"
   ],
   "id": "5346b15a76ba9ed4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 40\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStandard Deviation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mstd(total_rewards)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Test your agent\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[43mtest_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[12], line 16\u001B[0m, in \u001B[0;36mtest_agent\u001B[1;34m(agent, env, num_episodes)\u001B[0m\n\u001B[0;32m     13\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m---> 16\u001B[0m     \u001B[43mStateVisualizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay_rendered_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43movercooked_state\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mwindow_display\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmdp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mterrain_mtx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mget_action(\u001B[38;5;28mtuple\u001B[39m(obs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth_agent_obs\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m obs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth_agent_obs\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m     18\u001B[0m     next_obs, reward, terminated, event_info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Autonomous\\overcooked_ai\\src\\overcooked_ai_py\\visualization\\state_visualizer.py:258\u001B[0m, in \u001B[0;36mStateVisualizer.display_rendered_state\u001B[1;34m(self, state, hud_data, action_probs, grid, img_path, ipython_display, window_display)\u001B[0m\n\u001B[0;32m    255\u001B[0m     show_image_in_ipython(img_path)\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m window_display:\n\u001B[1;32m--> 258\u001B[0m     \u001B[43mrun_static_resizeable_window\u001B[49m\u001B[43m(\u001B[49m\u001B[43msurface\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow_fps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img_path\n",
      "File \u001B[1;32m~\\PycharmProjects\\Autonomous\\overcooked_ai\\src\\overcooked_ai_py\\visualization\\pygame_utils.py:21\u001B[0m, in \u001B[0;36mrun_static_resizeable_window\u001B[1;34m(surface, fps)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     20\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[1;32m---> 21\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[43mpygame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m event\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m QUIT:\n\u001B[0;32m     23\u001B[0m         pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mquit()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8adea1e3ba923fd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
