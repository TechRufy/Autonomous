{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"initial_id\",\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true,\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-19T07:48:29.054695Z\",\n",
    "     \"start_time\": \"2025-07-19T07:48:25.848215Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"from collections import defaultdict\\n\",\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import math\\n\",\n",
    "    \"import itertools\\n\",\n",
    "    \"\\n\",\n",
    "    \"from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, Overcooked\\n\",\n",
    "    \"from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\\n\",\n",
    "    \"from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\\n\",\n",
    "    \"from tqdm import tqdm\\n\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 1\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-19T07:48:29.086695Z\",\n",
    "     \"start_time\": \"2025-07-19T07:48:29.070697Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class Overcooked_Qlearn:\\n\",\n",
    "    \"    def __init__(\\n\",\n",
    "    \"        self,\\n\",\n",
    "    \"        layout_name: str,\\n\",\n",
    "    \"        learning_rate: float,\\n\",\n",
    "    \"        initial_epsilon: float,\\n\",\n",
    "    \"        epsilon_decay: float,\\n\",\n",
    "    \"        final_epsilon: float,\\n\",\n",
    "    \"        n_episodes,\\n\",\n",
    "    \"        discount_factor: float = 0.95,\\n\",\n",
    "    \"    ):\\n\",\n",
    "    \"        \\\"\\\"\\\"Initialize a Q-Learning agent.\\n\",\n",
    "    \"\\n\",\n",
    "    \"        Args:\\n\",\n",
    "    \"            env: The training environment\\n\",\n",
    "    \"            learning_rate: How quickly to update Q-values (0-1)\\n\",\n",
    "    \"            initial_epsilon: Starting exploration rate (usually 1.0)\\n\",\n",
    "    \"            epsilon_decay: How much to reduce epsilon each episode\\n\",\n",
    "    \"            final_epsilon: Minimum exploration rate (usually 0.1)\\n\",\n",
    "    \"            discount_factor: How much to value future rewards (0-1)\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        base_mdp = OvercookedGridworld.from_layout_name(layout_name) # or other layout\\n\",\n",
    "    \"        base_env = OvercookedEnv.from_mdp(base_mdp, info_level=0, horizon=400)\\n\",\n",
    "    \"        env = Overcooked(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.env = env\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.possible_action = [*itertools.product(range(6),repeat=2)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Q-table: maps (state, action) to expected reward\\n\",\n",
    "    \"        # defaultdict automatically creates entries with zeros for new states\\n\",\n",
    "    \"        self.q_values = defaultdict(lambda: np.zeros(len(self.possible_action)))\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.lr = learning_rate\\n\",\n",
    "    \"        self.discount_factor = discount_factor  # How much we care about future rewards\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Exploration parameters\\n\",\n",
    "    \"        self.epsilon = initial_epsilon\\n\",\n",
    "    \"        self.epsilon_decay = epsilon_decay\\n\",\n",
    "    \"        self.final_epsilon = final_epsilon\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Track learning progress\\n\",\n",
    "    \"        self.training_error = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def get_action(self, obs):\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # With probability epsilon: explore (random action)\\n\",\n",
    "    \"        if np.random.random() < self.epsilon:\\n\",\n",
    "    \"            return self.env.action_space.sample(),self.env.action_space.sample()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # With probability (1-epsilon): exploit (best known action)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            return self.possible_action[np.argmax(self.q_values[obs])]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def update(\\n\",\n",
    "    \"        self,\\n\",\n",
    "    \"        obs,\\n\",\n",
    "    \"        action,\\n\",\n",
    "    \"        reward,\\n\",\n",
    "    \"        terminated,\\n\",\n",
    "    \"        next_obs,\\n\",\n",
    "    \"    ):\\n\",\n",
    "    \"        \\\"\\\"\\\"Update Q-value based on experience.\\n\",\n",
    "    \"\\n\",\n",
    "    \"        This is the heart of Q-learning: learn from (state, action, reward, next_state)\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        # What's the best we could do from the next state?\\n\",\n",
    "    \"        # (Zero if episode terminated - no future rewards possible)\\n\",\n",
    "    \"        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # What should the Q-value be? (Bellman equation)\\n\",\n",
    "    \"        target = reward + self.discount_factor * future_q_value\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # How wrong was our current estimate?\\n\",\n",
    "    \"        temporal_difference = target - self.q_values[obs][self.possible_action.index(action)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Update our estimate in the direction of the error\\n\",\n",
    "    \"        # Learning rate controls how big steps we take\\n\",\n",
    "    \"        self.q_values[obs][self.possible_action.index(action)] = (\\n\",\n",
    "    \"            self.q_values[obs][self.possible_action.index(action)] + self.lr * temporal_difference\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Track learning progress (useful for debugging)\\n\",\n",
    "    \"        self.training_error.append(temporal_difference)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def decay_epsilon(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"Reduce exploration rate after each episode.\\\"\\\"\\\"\\n\",\n",
    "    \"        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\"\n",
    "   ],\n",
    "   \"id\": \"9356720f48fadce6\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 2\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-19T07:48:29.504218Z\",\n",
    "     \"start_time\": \"2025-07-19T07:48:29.458696Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Training hyperparameters\\n\",\n",
    "    \"learning_rate = 0.02        # How fast to learn (higher = faster but less stable)\\n\",\n",
    "    \"n_episodes = 5000        # Number of hands to practice\\n\",\n",
    "    \"start_epsilon = 1.0         # Start with 100% random actions\\n\",\n",
    "    \"epsilon_decay = start_epsilon / (n_episodes *4)  # Reduce exploration over time\\n\",\n",
    "    \"final_epsilon = 0.2         # Always keep some exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"agent = Overcooked_Qlearn(\\n\",\n",
    "    \"    layout_name=\\\"cramped_room\\\",\\n\",\n",
    "    \"    learning_rate=learning_rate,\\n\",\n",
    "    \"    initial_epsilon=start_epsilon,\\n\",\n",
    "    \"    epsilon_decay=epsilon_decay,\\n\",\n",
    "    \"    final_epsilon=final_epsilon,\\n\",\n",
    "    \"    n_episodes=n_episodes,\\n\",\n",
    "    \")\"\n",
    "   ],\n",
    "   \"id\": \"9e9d9733a92998a6\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 3\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-19T15:05:16.376490Z\",\n",
    "     \"start_time\": \"2025-07-19T15:04:27.106022Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"  # Progress bar\\n\",\n",
    "    \"n_episodes = 100\\n\",\n",
    "    \"for episode in tqdm(range(n_episodes)):\\n\",\n",
    "    \"    obs = agent.env.reset()\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Play one complete hand\\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        # Agent chooses action (initially random, gradually more intelligent)\\n\",\n",
    "    \"        action = agent.get_action(tuple(obs['both_agent_obs'][0] + obs['both_agent_obs'][1]))\\n\",\n",
    "    \"        StateVisualizer().display_rendered_state(obs['overcooked_state'],window_display=True,grid=agent.env.mdp.terrain_mtx)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Take action and observe result\\n\",\n",
    "    \"        next_obs, reward, terminated, event_info = agent.env.step(action)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if terminated:\\n\",\n",
    "    \"            subgoal_reward = event_info['episode'][\\\"ep_shaped_r\\\"]\\n\",\n",
    "    \"            reward = reward + (subgoal_reward - int(subgoal_reward/ ((episode + 10)*10)))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Learn from this experience\\n\",\n",
    "    \"        agent.update(tuple(obs['both_agent_obs'][0] + obs['both_agent_obs'][1]),\\n\",\n",
    "    \"                     action, reward, terminated,\\n\",\n",
    "    \"                     tuple(next_obs['both_agent_obs'][0] + next_obs['both_agent_obs'][1]))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"        #StateVisualizer().display_rendered_state(next_obs['overcooked_state'],window_display=True,grid=agent.env.mdp.terrain_mtx)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Move to next state\\n\",\n",
    "    \"        done = terminated\\n\",\n",
    "    \"        obs = next_obs\\n\",\n",
    "    \"    # Reduce exploration rate (agent becomes less random over time)\\n\",\n",
    "    \"    agent.decay_epsilon()\"\n",
    "   ],\n",
    "   \"id\": \"a63e87f97b77b34c\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"  0%|          | 0/100 [00:49<?, ?it/s]\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"ename\": \"KeyboardInterrupt\",\n",
    "     \"evalue\": \"\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001B[1;31m---------------------------------------------------------------------------\\u001B[0m\",\n",
    "      \"\\u001B[1;31mKeyboardInterrupt\\u001B[0m                         Traceback (most recent call last)\",\n",
    "      \"Cell \\u001B[1;32mIn[13], line 11\\u001B[0m\\n\\u001B[0;32m      8\\u001B[0m \\u001B[38;5;28;01mwhile\\u001B[39;00m \\u001B[38;5;129;01mnot\\u001B[39;00m done:\\n\\u001B[0;32m      9\\u001B[0m     \\u001B[38;5;66;03m# Agent chooses action (initially random, gradually more intelligent)\\u001B[39;00m\\n\\u001B[0;32m     10\\u001B[0m     action \\u001B[38;5;241m=\\u001B[39m agent\\u001B[38;5;241m.\\u001B[39mget_action(\\u001B[38;5;28mtuple\\u001B[39m(obs[\\u001B[38;5;124m'\\u001B[39m\\u001B[38;5;124mboth_agent_obs\\u001B[39m\\u001B[38;5;124m'\\u001B[39m][\\u001B[38;5;241m0\\u001B[39m] \\u001B[38;5;241m+\\u001B[39m obs[\\u001B[38;5;124m'\\u001B[39m\\u001B[38;5;124mboth_agent_obs\\u001B[39m\\u001B[38;5;124m'\\u001B[39m][\\u001B[38;5;241m1\\u001B[39m]))\\n\\u001B[1;32m---> 11\\u001B[0m     \\u001B[43mStateVisualizer\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mdisplay_rendered_state\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mobs\\u001B[49m\\u001B[43m[\\u001B[49m\\u001B[38;5;124;43m'\\u001B[39;49m\\u001B[38;5;124;43movercooked_state\\u001B[39;49m\\u001B[38;5;124;43m'\\u001B[39;49m\\u001B[43m]\\u001B[49m\\u001B[43m,\\u001B[49m\\u001B[43mwindow_display\\u001B[49m\\u001B[38;5;241;43m=\\u001B[39;49m\\u001B[38;5;28;43;01mTrue\\u001B[39;49;00m\\u001B[43m,\\u001B[49m\\u001B[43mgrid\\u001B[49m\\u001B[38;5;241;43m=\\u001B[39;49m\\u001B[43magent\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43menv\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mmdp\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mterrain_mtx\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[0;32m     14\\u001B[0m     \\u001B[38;5;66;03m# Take action and observe result\\u001B[39;00m\\n\\u001B[0;32m     15\\u001B[0m     next_obs, reward, terminated, event_info \\u001B[38;5;241m=\\u001B[39m agent\\u001B[38;5;241m.\\u001B[39menv\\u001B[38;5;241m.\\u001B[39mstep(action)\\n\",\n",
    "      \"File \\u001B[1;32m~\\\\PycharmProjects\\\\Autonomous\\\\overcooked_ai\\\\src\\\\overcooked_ai_py\\\\visualization\\\\state_visualizer.py:258\\u001B[0m, in \\u001B[0;36mStateVisualizer.display_rendered_state\\u001B[1;34m(self, state, hud_data, action_probs, grid, img_path, ipython_display, window_display)\\u001B[0m\\n\\u001B[0;32m    255\\u001B[0m     show_image_in_ipython(img_path)\\n\\u001B[0;32m    257\\u001B[0m \\u001B[38;5;28;01mif\\u001B[39;00m window_display:\\n\\u001B[1;32m--> 258\\u001B[0m     \\u001B[43mrun_static_resizeable_window\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43msurface\\u001B[49m\\u001B[43m,\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[38;5;28;43mself\\u001B[39;49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mwindow_fps\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[0;32m    260\\u001B[0m \\u001B[38;5;28;01mreturn\\u001B[39;00m img_path\\n\",\n",
    "      \"File \\u001B[1;32m~\\\\PycharmProjects\\\\Autonomous\\\\overcooked_ai\\\\src\\\\overcooked_ai_py\\\\visualization\\\\pygame_utils.py:21\\u001B[0m, in \\u001B[0;36mrun_static_resizeable_window\\u001B[1;34m(surface, fps)\\u001B[0m\\n\\u001B[0;32m     19\\u001B[0m \\u001B[38;5;28;01mwhile\\u001B[39;00m \\u001B[38;5;28;01mTrue\\u001B[39;00m:\\n\\u001B[0;32m     20\\u001B[0m     pygame\\u001B[38;5;241m.\\u001B[39mevent\\u001B[38;5;241m.\\u001B[39mpump()\\n\\u001B[1;32m---> 21\\u001B[0m     event \\u001B[38;5;241m=\\u001B[39m \\u001B[43mpygame\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mevent\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mwait\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[0;32m     22\\u001B[0m     \\u001B[38;5;28;01mif\\u001B[39;00m event\\u001B[38;5;241m.\\u001B[39mtype \\u001B[38;5;241m==\\u001B[39m QUIT:\\n\\u001B[0;32m     23\\u001B[0m         pygame\\u001B[38;5;241m.\\u001B[39mdisplay\\u001B[38;5;241m.\\u001B[39mquit()\\n\",\n",
    "      \"\\u001B[1;31mKeyboardInterrupt\\u001B[0m: \"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 13\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-19T15:04:11.220747Z\",\n",
    "     \"start_time\": \"2025-07-19T15:03:42.490718Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Test the trained agent\\n\",\n",
    "    \"def test_agent(agent, env, num_episodes=1000):\\n\",\n",
    "    \"    \\\"\\\"\\\"Test agent performance without learning or exploration.\\\"\\\"\\\"\\n\",\n",
    "    \"    total_rewards = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Temporarily disable exploration for testing\\n\",\n",
    "    \"    old_epsilon = agent.epsilon\\n\",\n",
    "    \"    agent.epsilon = 0.0  # Pure exploitation\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for episode in tqdm(range(num_episodes)):\\n\",\n",
    "    \"        obs = env.reset()\\n\",\n",
    "    \"        episode_reward = 0\\n\",\n",
    "    \"        done = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"        while not done:\\n\",\n",
    "    \"            StateVisualizer().display_rendered_state(obs['overcooked_state'],window_display=True,grid=agent.env.mdp.terrain_mtx)\\n\",\n",
    "    \"            action = agent.get_action(tuple(obs['both_agent_obs'][0] + obs['both_agent_obs'][1]))\\n\",\n",
    "    \"            next_obs, reward, terminated, event_info = env.step(action)\\n\",\n",
    "    \"            if reward > 0:\\n\",\n",
    "    \"                print(reward)\\n\",\n",
    "    \"                print(\\\"soup delivered\\\")\\n\",\n",
    "    \"            episode_reward += reward\\n\",\n",
    "    \"            done = terminated\\n\",\n",
    "    \"            obs = next_obs\\n\",\n",
    "    \"\\n\",\n",
    "    \"        total_rewards.append(episode_reward)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Restore original epsilon\\n\",\n",
    "    \"    agent.epsilon = old_epsilon\\n\",\n",
    "    \"\\n\",\n",
    "    \"    win_rate = np.mean(np.array(total_rewards) > 0)\\n\",\n",
    "    \"    average_reward = np.mean(total_rewards)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"Test Results over {num_episodes} episodes:\\\")\\n\",\n",
    "    \"    print(f\\\"Win Rate: {win_rate:.1%}\\\")\\n\",\n",
    "    \"    print(f\\\"Average Reward: {average_reward:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"Standard Deviation: {np.std(total_rewards):.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test your agent\\n\",\n",
    "    \"test_agent(agent, agent.env)\"\n",
    "   ],\n",
    "   \"id\": \"5346b15a76ba9ed4\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"  0%|          | 0/1000 [00:28<?, ?it/s]\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"ename\": \"KeyboardInterrupt\",\n",
    "     \"evalue\": \"\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001B[1;31m---------------------------------------------------------------------------\\u001B[0m\",\n",
    "      \"\\u001B[1;31mKeyboardInterrupt\\u001B[0m                         Traceback (most recent call last)\",\n",
    "      \"Cell \\u001B[1;32mIn[12], line 40\\u001B[0m\\n\\u001B[0;32m     37\\u001B[0m     \\u001B[38;5;28mprint\\u001B[39m(\\u001B[38;5;124mf\\u001B[39m\\u001B[38;5;124m\\\"\\u001B[39m\\u001B[38;5;124mStandard Deviation: \\u001B[39m\\u001B[38;5;132;01m{\\u001B[39;00mnp\\u001B[38;5;241m.\\u001B[39mstd(total_rewards)\\u001B[38;5;132;01m:\\u001B[39;00m\\u001B[38;5;124m.3f\\u001B[39m\\u001B[38;5;132;01m}\\u001B[39;00m\\u001B[38;5;124m\\\"\\u001B[39m)\\n\\u001B[0;32m     39\\u001B[0m \\u001B[38;5;66;03m# Test your agent\\u001B[39;00m\\n\\u001B[1;32m---> 40\\u001B[0m \\u001B[43mtest_agent\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43magent\\u001B[49m\\u001B[43m,\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[43magent\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43menv\\u001B[49m\\u001B[43m)\\u001B[49m\\n\",\n",
    "      \"Cell \\u001B[1;32mIn[12], line 16\\u001B[0m, in \\u001B[0;36mtest_agent\\u001B[1;34m(agent, env, num_episodes)\\u001B[0m\\n\\u001B[0;32m     13\\u001B[0m done \\u001B[38;5;241m=\\u001B[39m \\u001B[38;5;28;01mFalse\\u001B[39;00m\\n\\u001B[0;32m     15\\u001B[0m \\u001B[38;5;28;01mwhile\\u001B[39;00m \\u001B[38;5;129;01mnot\\u001B[39;00m done:\\n\\u001B[1;32m---> 16\\u001B[0m     \\u001B[43mStateVisualizer\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mdisplay_rendered_state\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mobs\\u001B[49m\\u001B[43m[\\u001B[49m\\u001B[38;5;124;43m'\\u001B[39;49m\\u001B[38;5;124;43movercooked_state\\u001B[39;49m\\u001B[38;5;124;43m'\\u001B[39;49m\\u001B[43m]\\u001B[49m\\u001B[43m,\\u001B[49m\\u001B[43mwindow_display\\u001B[49m\\u001B[38;5;241;43m=\\u001B[39;49m\\u001B[38;5;28;43;01mTrue\\u001B[39;49;00m\\u001B[43m,\\u001B[49m\\u001B[43mgrid\\u001B[49m\\u001B[38;5;241;43m=\\u001B[39;49m\\u001B[43magent\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43menv\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mmdp\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mterrain_mtx\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[0;32m     17\\u001B[0m     action \\u001B[38;5;241m=\\u001B[39m agent\\u001B[38;5;241m.\\u001B[39mget_action(\\u001B[38;5;28mtuple\\u001B[39m(obs[\\u001B[38;5;124m'\\u001B[39m\\u001B[38;5;124mboth_agent_obs\\u001B[39m\\u001B[38;5;124m'\\u001B[39m][\\u001B[38;5;241m0\\u001B[39m] \\u001B[38;5;241m+\\u001B[39m obs[\\u001B[38;5;124m'\\u001B[39m\\u001B[38;5;124mboth_agent_obs\\u001B[39m\\u001B[38;5;124m'\\u001B[39m][\\u001B[38;5;241m1\\u001B[39m]))\\n\\u001B[0;32m     18\\u001B[0m     next_obs, reward, terminated, event_info \\u001B[38;5;241m=\\u001B[39m env\\u001B[38;5;241m.\\u001B[39mstep(action)\\n\",\n",
    "      \"File \\u001B[1;32m~\\\\PycharmProjects\\\\Autonomous\\\\overcooked_ai\\\\src\\\\overcooked_ai_py\\\\visualization\\\\state_visualizer.py:258\\u001B[0m, in \\u001B[0;36mStateVisualizer.display_rendered_state\\u001B[1;34m(self, state, hud_data, action_probs, grid, img_path, ipython_display, window_display)\\u001B[0m\\n\\u001B[0;32m    255\\u001B[0m     show_image_in_ipython(img_path)\\n\\u001B[0;32m    257\\u001B[0m \\u001B[38;5;28;01mif\\u001B[39;00m window_display:\\n\\u001B[1;32m--> 258\\u001B[0m     \\u001B[43mrun_static_resizeable_window\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43msurface\\u001B[49m\\u001B[43m,\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[38;5;28;43mself\\u001B[39;49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mwindow_fps\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[0;32m    260\\u001B[0m \\u001B[38;5;28;01mreturn\\u001B[39;00m img_path\\n\",\n",
    "      \"File \\u001B[1;32m~\\\\PycharmProjects\\\\Autonomous\\\\overcooked_ai\\\\src\\\\overcooked_ai_py\\\\visualization\\\\pygame_utils.py:21\\u001B[0m, in \\u001B[0;36mrun_static_resizeable_window\\u001B[1;34m(surface, fps)\\u001B[0m\\n\\u001B[0;32m     19\\u001B[0m \\u001B[38;5;28;01mwhile\\u001B[39;00m \\u001B[38;5;28;01mTrue\\u001B[39;00m:\\n\\u001B[0;32m     20\\u001B[0m     pygame\\u001B[38;5;241m.\\u001B[39mevent\\u001B[38;5;241m.\\u001B[39mpump()\\n\\u001B[1;32m---> 21\\u001B[0m     event \\u001B[38;5;241m=\\u001B[39m \\u001B[43mpygame\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mevent\\u001B[49m\\u001B[38;5;241;43m.\\u001B[39;49m\\u001B[43mwait\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[0;32m     22\\u001B[0m     \\u001B[38;5;28;01mif\\u001B[39;00m event\\u001B[38;5;241m.\\u001B[39mtype \\u001B[38;5;241m==\\u001B[39m QUIT:\\n\\u001B[0;32m     23\\u001B[0m         pygame\\u001B[38;5;241m.\\u001B[39mdisplay\\u001B[38;5;241m.\\u001B[39mquit()\\n\",\n",
    "      \"\\u001B[1;31mKeyboardInterrupt\\u001B[0m: \"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 12\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": \"\",\n",
    "   \"id\": \"8adea1e3ba923fd0\"\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "8cb43374d847dc58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
