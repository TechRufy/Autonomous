{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-20T23:01:50.577255Z",
     "start_time": "2025-07-20T23:01:48.186346Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statistics\n",
    "import torch.nn.functional as F\n",
    "from exceptiongroup import catch\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, Overcooked\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T23:05:39.389268Z",
     "start_time": "2025-07-20T23:05:39.381269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OvercookedRewardShaping(Overcooked):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def step(self, actions):\n",
    "        observation, base_reward, done, info = super().step(actions)\n",
    "        if base_reward != 0:\n",
    "            print(\"Soup delivered! Voto: {}\".format(base_reward)) # base_reward is 20 if soup is delivered\n",
    "        shaped_reward = base_reward + self._compute_shaping(observation['both_agent_obs'])\n",
    "        return observation, shaped_reward, done, info\n",
    "\n",
    "    def _compute_shaping(self, observations):\n",
    "        shaping = 0\n",
    "        for obs in observations:\n",
    "            holding_vector = obs[4:8]\n",
    "            holding_soup = obs[5:6]\n",
    "            soup_full_cooking_ready = obs[24:27]\n",
    "            soup_empty = obs[23:24]\n",
    "            soup_cooking = obs[25:26]\n",
    "            pot_onions = obs[27:28]\n",
    "\n",
    "            # Penalty if holding an object\n",
    "            #if holding_vector.any():\n",
    "            #    shaping -= 0.05\n",
    "            # Reward if holding a soup\n",
    "            #if holding_soup.any():\n",
    "            #    shaping += 0.1\n",
    "            # Reward if soup is full/cooking/ready\n",
    "            if soup_cooking.any():\n",
    "                shaping += 0.3\n",
    "            # Reward if onion are putted into the soup\n",
    "            #if soup_empty.any():\n",
    "            # shaping += int(pot_onions)*0.01\n",
    "\n",
    "        return shaping"
   ],
   "id": "3cd2d20dfbbb0e54",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T00:44:02.753930Z",
     "start_time": "2025-07-21T00:44:02.710448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pygame\n",
    "\n",
    "\n",
    "class Approximator(nn.Module):\n",
    "    def __init__(self, action_size,input_size):\n",
    "        super(Approximator, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=input_size, out_features=64) # You'll need to specify input features here\n",
    "        self.dense2 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dense3 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dense4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.policy_logits = nn.Linear(in_features=64, out_features=action_size)\n",
    "        self.value = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.dense1(state))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = F.relu(self.dense3(x))\n",
    "        x = F.relu(self.dense4(x))\n",
    "        x = F.relu(self.dense5(x))\n",
    "        logits = self.policy_logits(x)\n",
    "        value = self.value(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "\n",
    "class OvercookedPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layout_name,\n",
    "        model_class,\n",
    "        gamma,  # Discount factor\n",
    "        lr_actor,  # Actor learning rate\n",
    "        lr_critic,  # Critic learning rate\n",
    "        clip_ratio, # PPO clip ratio\n",
    "        epochs, # Number of optimization epochs\n",
    "        batch_size,\n",
    "        optimizer_class,\n",
    "        lmbda\n",
    "\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.individual_action_values = Action.ALL_ACTIONS\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "\n",
    "\n",
    "        base_mdp = OvercookedGridworld.from_layout_name(layout_name) # or other layout\n",
    "        base_env = OvercookedEnv.from_mdp(base_mdp, info_level=0, horizon=400)\n",
    "        #env = Overcooked(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "        self.env  = OvercookedRewardShaping(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "\n",
    "        self.n_possible_action = self.env.action_space.n\n",
    "\n",
    "        dummy_state = self.env.reset()\n",
    "        dummy_obs_agent0 = dummy_state['both_agent_obs'][0]\n",
    "        state_input_size = len(dummy_obs_agent0) # Assuming the featurized state is a flat vector\n",
    "\n",
    "        self.actor_model = model_class(self.n_possible_action, state_input_size).to(self.device)\n",
    "        self.critic_model = model_class(self.n_possible_action, state_input_size).to(self.device)\n",
    "        self.actor_optimizer = optimizer_class(self.actor_model.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optimizer_class(self.critic_model.parameters(), lr=lr_actor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def ppo_loss(self,old_logits, old_values, returns, states, actions,dones):\n",
    "\n",
    "        old_logits0_initial, old_logits1_initial = old_logits\n",
    "        old_values0_initial, old_values1_initial = old_values\n",
    "\n",
    "        def get_advantages_gae(values_tensor, masks_tensor, rewards_tensor):\n",
    "            # values_tensor should be (batch_size, 1), squeeze it\n",
    "            values_squeezed = values_tensor.squeeze(-1) # shape (batch_size,)\n",
    "\n",
    "            returns = torch.zeros_like(rewards_tensor).to(self.device) # Initialize returns on GPU\n",
    "            gae = 0\n",
    "\n",
    "            advantages_tensor = torch.zeros_like(rewards_tensor).to(self.device)\n",
    "            last_gae_lam = 0\n",
    "            for t in reversed(range(len(rewards_tensor))):\n",
    "                if t == len(rewards_tensor) - 1: # Last step in the collected trajectory\n",
    "\n",
    "                    next_value = 0.0 # If episode is truly done, next value is 0\n",
    "                else:\n",
    "                    next_value = values_squeezed[t+1] # V(s_{t+1})\n",
    "\n",
    "                delta = rewards_tensor[t] + self.gamma * next_value * masks_tensor[t] - values_squeezed[t]\n",
    "                last_gae_lam = delta + self.gamma * self.lmbda * masks_tensor[t] * last_gae_lam\n",
    "                advantages_tensor[t] = last_gae_lam\n",
    "\n",
    "            returns_gae = advantages_tensor + values_squeezed\n",
    "\n",
    "            # Normalize advantages\n",
    "            advantages_normalized = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-10)\n",
    "\n",
    "            return returns_gae, advantages_normalized\n",
    "\n",
    "\n",
    "        def compute_loss(logits, values, actions, returns,old_logits,advantages):\n",
    "            actions_onehot = F.one_hot(actions.long(), num_classes=self.n_possible_action).float()\n",
    "\n",
    "            policy = F.softmax(logits, dim=-1)\n",
    "            action_probs = torch.sum(actions_onehot * policy, dim=-1) # Use dim=-1 for last dimension\n",
    "\n",
    "            old_policy = F.softmax(old_logits.detach(), dim=-1)\n",
    "            old_action_probs = torch.sum(actions_onehot * old_policy, dim=-1)\n",
    "\n",
    "            epsilon = 1e-10\n",
    "            # Policy loss\n",
    "\n",
    "            advantages_tensor = torch.tensor(advantages, dtype=torch.long).to(self.device)\n",
    "            ratio = torch.exp(torch.log(action_probs + 1e-10) - torch.log(old_action_probs + 1e-10))\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "            policy_loss = torch.mean(torch.min(ratio * advantages_tensor, clipped_ratio * advantages_tensor))\n",
    "\n",
    "            # Value loss\n",
    "            returns_tensor = torch.tensor(returns, dtype=torch.long).to(self.device)\n",
    "            value_loss = torch.mean(torch.square(values.squeeze(-1) - returns_tensor)) # Squeeze value to match returns shape\n",
    "\n",
    "            # Entropy bonus (optional)\n",
    "            # Ensure policy is not zero for log\n",
    "            entropy_bonus = torch.mean(policy * torch.log(policy + epsilon)) # PyTorch's entropy loss is typically negative\n",
    "\n",
    "            total_loss = policy_loss + 0.1 * entropy_bonus # Note: In PyTorch, entropy_bonus typically added for maximization, so + sign. If you want regularization that penalizes low entropy, it's typically subtracted like in TF. Let's keep your original -0.01 for regularization.\n",
    "            return total_loss, value_loss\n",
    "\n",
    "        def train_step(states, actions, returns, old_logits, old_values, advantages_tuple):\n",
    "            self.actor_optimizer.zero_grad() # Zero gradients for this optimization step\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            logits0, _ = self.actor_model(states[0])\n",
    "            _, values0 = self.critic_model(states[0])\n",
    "            logits1, _ = self.actor_model(states[1])\n",
    "            _, values1 = self.critic_model(states[1])\n",
    "\n",
    "            policy_loss0, value_loss0 = compute_loss(logits0, values0, actions[0], returns, old_logits[0], advantages_tuple[0])\n",
    "\n",
    "            policy_loss1, value_loss1 = compute_loss(logits1, values1, actions[1], returns, old_logits[1], advantages_tuple[1])\n",
    "\n",
    "            total_value_loss = value_loss0 + value_loss1\n",
    "            total_policy_loss = policy_loss0 + policy_loss1\n",
    "\n",
    "            total_value_loss.backward()\n",
    "            total_policy_loss.backward() # Compute gradients\n",
    "\n",
    "            self.actor_optimizer.step() # Zero gradients for this optimization step\n",
    "            self.critic_optimizer.step() # Update model parameters\n",
    "\n",
    "            return total_value_loss.item(), total_policy_loss.item() # Return scalar loss values\n",
    "\n",
    "        current_loss0, current_loss1 = 0, 0\n",
    "        rewards_tensor = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "        masks_tensor = torch.tensor(dones, dtype=torch.float32).to(self.device) # `dones_list` is `not done` (1 for non-terminal, 0 for terminal)\n",
    "        returns0_gae, advantages0 = get_advantages_gae(old_values0_initial, masks_tensor, rewards_tensor)\n",
    "        returns1_gae, advantages1 = get_advantages_gae(old_values1_initial, masks_tensor, rewards_tensor)\n",
    "        for _ in range(self.epochs):\n",
    "            # Pass all initial data, the inner train_step will recompute current model outputs\n",
    "            current_loss0, current_loss1 = train_step(\n",
    "                (states[0], states[1]),\n",
    "                (actions[0], actions[1]),\n",
    "                returns,\n",
    "                (old_logits0_initial, old_logits1_initial),\n",
    "                (old_values0_initial, old_values1_initial),\n",
    "                (advantages0, advantages1)\n",
    "            )\n",
    "\n",
    "        return current_loss0, current_loss1\n",
    "\n",
    "\n",
    "    def trainingLoop(self,max_episodes,max_steps_per_episode):\n",
    "        for episode in range(max_episodes):\n",
    "            states0,states1, actions0, actions1, rewards, values1,values0, returns,dones = [], [], [], [], [],[],[],[],[]\n",
    "            state = self.env.reset()\n",
    "            for step in range(max_steps_per_episode):\n",
    "                state0_tensor = torch.tensor(state['both_agent_obs'][0], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                state1_tensor = torch.tensor(state['both_agent_obs'][1], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "                self.actor_model.eval()\n",
    "                self.critic_model.eval()\n",
    "                with torch.no_grad(): # No need to track gradients for action selection\n",
    "                    logits0, _ = self.actor_model(state0_tensor)\n",
    "                    _, value0 = self.critic_model(state0_tensor)\n",
    "                    logits1, _ = self.actor_model(state1_tensor)\n",
    "                    _, value1 = self.critic_model(state1_tensor)\n",
    "                self.actor_model.train()\n",
    "                self.critic_model.train() # Sw\n",
    "\n",
    "                action_dist0 = torch.distributions.Categorical(logits=logits0)\n",
    "                action0 = action_dist0.sample().item() # Get scalar action\n",
    "\n",
    "                action_dist1 = torch.distributions.Categorical(logits=logits1)\n",
    "                action1 = action_dist1.sample().item()\n",
    "\n",
    "                action = (action0, action1)\n",
    "                next_state, reward, done, event_info = self.env.step(action)\n",
    "\n",
    "                states0.append(state0_tensor)\n",
    "                states1.append(state1_tensor)\n",
    "                actions0.append(action0)\n",
    "                actions1.append(action1)\n",
    "                rewards.append(reward)\n",
    "                values0.append(value0)\n",
    "                values1.append(value1)\n",
    "                dones.append(not done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "\n",
    "\n",
    "                    # Convert lists of tensors/numpy arrays to batched tensors\n",
    "                    states0_batch = torch.cat(states0, dim=0).to(self.device)\n",
    "                    states1_batch = torch.cat(states1, dim=0).to(self.device)\n",
    "\n",
    "                    actions0_batch = torch.tensor(actions0, dtype=torch.long).to(self.device) # Actions should be long for indexing/one-hot\n",
    "                    actions1_batch = torch.tensor(actions1, dtype=torch.long).to(self.device)\n",
    "\n",
    "                    # Values are already tensors from model output, concatenate them\n",
    "                    values0_batch = torch.cat(values0, dim=0).to(self.device)\n",
    "                    values1_batch = torch.cat(values1, dim=0).to(self.device)\n",
    "\n",
    "                    old_logits0_batch, _ = self.actor_model(states0_batch)\n",
    "                    old_logits1_batch, _ = self.actor_model(states1_batch)\n",
    "\n",
    "                    total_value_loss, total_policy_loss = self.ppo_loss(\n",
    "                        (old_logits0_batch, old_logits1_batch),\n",
    "                        (values0_batch, values1_batch),\n",
    "                        rewards,\n",
    "                        (states0_batch, states1_batch),\n",
    "                        (actions0_batch, actions1_batch),\n",
    "                        dones\n",
    "                    )\n",
    "                    if episode % 10 == 0:\n",
    "                        print(f\"Episode: {episode + 1}, value_loss : {total_value_loss}, policy_loss : {total_policy_loss}, average reward {statistics.fmean(rewards)}\")\n",
    "\n",
    "                    break\n",
    "    def test(self, n_episodes, visualize=False, print_action=False):\n",
    "        pygame.init()\n",
    "        visualizer = StateVisualizer()\n",
    "\n",
    "        # 2) Grab your grid and do one dummy render to get a surface\n",
    "        grid = self.env.base_env.mdp.terrain_mtx\n",
    "        _ = self.env.reset()\n",
    "        surf = visualizer.render_state(self.env.base_env.state, grid=grid)\n",
    "\n",
    "        # 3) Use that surface’s size for your window\n",
    "        win_w, win_h = surf.get_size()\n",
    "        screen = pygame.display.set_mode((win_w, win_h), pygame.RESIZABLE)\n",
    "        clock  = pygame.time.Clock()\n",
    "\n",
    "        running = True\n",
    "        obs = self.env.reset() #observation of the starting state\n",
    "        soup_delivered = 0\n",
    "\n",
    "        total_rewards = []\n",
    "        while running:\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            self.actor_model.eval()\n",
    "            self.critic_model.eval() # Set model to evaluation mode\n",
    "            with torch.no_grad(): # No gradient calculation during testing\n",
    "                state0_tensor = torch.tensor(obs['both_agent_obs'][0], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                state1_tensor = torch.tensor(obs['both_agent_obs'][1], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "                logits0, _ = self.actor_model(state0_tensor)\n",
    "                logits1, _ = self.actor_model(state1_tensor)\n",
    "\n",
    "                action0 = torch.argmax(logits0, dim=1).item()\n",
    "                action1 = torch.argmax(logits1, dim=1).item()\n",
    "\n",
    "                if print_action:\n",
    "                    print(action0, action1)\n",
    "                # try to step; if episode is over, catch and reset\n",
    "                try:\n",
    "                    # Overcooked wrapper returns (obs_p0, obs_p1, reward, done, info)\n",
    "                    observation, reward, done, info = self.env.step((action0, action1))\n",
    "                    if reward > 19:\n",
    "                        soup_delivered += 1\n",
    "                except AssertionError:\n",
    "                    # base_env.is_done() was True → reset and continue\n",
    "                    self.env.reset()\n",
    "                    break\n",
    "\n",
    "                # render the new state\n",
    "                surf = visualizer.render_state(self.env.base_env.state, grid=grid)\n",
    "\n",
    "                # draw it\n",
    "                screen.blit(surf, (0, 0))\n",
    "                pygame.display.flip()\n",
    "\n",
    "                clock.tick(15)   # cap at 30 FPS\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "        print(f\"Soup delivered: {soup_delivered}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "240394b8b3aec7f1",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T00:44:03.358561Z",
     "start_time": "2025-07-21T00:44:03.333561Z"
    }
   },
   "cell_type": "code",
   "source": "OPPO = OvercookedPPO(\"cramped_room\",model_class =Approximator,gamma = 0.99,lr_actor = 0.0001,lr_critic = 0.0001,clip_ratio = 0.2 ,epochs = 15,batch_size = 64,optimizer_class = torch.optim.Adam,lmbda=0.95)",
   "id": "978e960c12810ac8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T00:45:46.717824Z",
     "start_time": "2025-07-21T00:45:01.039825Z"
    }
   },
   "cell_type": "code",
   "source": "OPPO.trainingLoop(max_episodes = 100,max_steps_per_episode = 2000)",
   "id": "f16eee44ecc0b7d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TechRufy\\AppData\\Local\\Temp\\ipykernel_24544\\1812919641.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  advantages_tensor = torch.tensor(advantages, dtype=torch.long).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, value_loss : 1.0180804110859754e-06, policy_loss : 0.29167038202285767, average reward 0.0285\n",
      "Episode: 11, value_loss : 7.719327186350711e-07, policy_loss : 0.36793777346611023, average reward 0.0285\n",
      "Episode: 21, value_loss : 5.586688303083065e-07, policy_loss : 0.3553040027618408, average reward 0.0285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[145], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mOPPO\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainingLoop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_episodes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmax_steps_per_episode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[141], line 185\u001B[0m, in \u001B[0;36mOvercookedPPO.trainingLoop\u001B[1;34m(self, max_episodes, max_steps_per_episode)\u001B[0m\n\u001B[0;32m    183\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_steps_per_episode):\n\u001B[1;32m--> 185\u001B[0m     state0_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mboth_agent_obs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m     state1_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth_agent_obs\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor_model\u001B[38;5;241m.\u001B[39meval()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T00:44:57.950533Z",
     "start_time": "2025-07-21T00:44:56.150631Z"
    }
   },
   "cell_type": "code",
   "source": "OPPO.test(n_episodes = 100,visualize = True,print_action = True)",
   "id": "2dcd2d4e41b17c76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "Soup delivered: 0\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f353440204a4a2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
