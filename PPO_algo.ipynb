{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T17:03:40.330812Z",
     "start_time": "2025-07-22T17:03:31.893737Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statistics\n",
    "import torch.nn.functional as F\n",
    "from exceptiongroup import catch\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, Overcooked\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T17:46:29.333813Z",
     "start_time": "2025-07-22T17:46:29.314312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OvercookedRewardShaping(Overcooked): # Using OvercookedGridworld if it's the base\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.prev_agent_obs = [None, None] # To store observations from previous step for both agents\n",
    "\n",
    "    def step(self, actions):\n",
    "        observation, base_reward, done, info = super().step(actions)\n",
    "\n",
    "        # Calculate shaped reward for each agent\n",
    "        shaped_reward_total = 0\n",
    "        current_agent_obs = observation['both_agent_obs']\n",
    "\n",
    "        # Ensure prev_agent_obs is initialized for the first step\n",
    "        if self.prev_agent_obs[0] is None:\n",
    "            self.prev_agent_obs = current_agent_obs\n",
    "\n",
    "        for i, obs in enumerate(current_agent_obs):\n",
    "            # Pass current and previous observation for this agent\n",
    "            shaped_reward_total += self._compute_agent_shaping(obs, self.prev_agent_obs[i])\n",
    "\n",
    "        # Update previous observations for the next step\n",
    "        self.prev_agent_obs = current_agent_obs\n",
    "\n",
    "        final_shaped_reward = base_reward + shaped_reward_total\n",
    "\n",
    "        # Optional: Print base reward only if it's non-zero for clarity\n",
    "        # if base_reward != 0:\n",
    "        #     print(f\"Soup delivered! Base Reward: {base_reward}. Shaped Reward Total: {shaped_reward_total}. Final: {final_shaped_reward}\")\n",
    "\n",
    "        return observation, final_shaped_reward, done, info\n",
    "\n",
    "    def _compute_agent_shaping(self, current_obs, prev_obs):\n",
    "        shaping = 0.0\n",
    "\n",
    "        ONION_IDX = 0\n",
    "        SOUP_IDX = 1\n",
    "        DISH_IDX = 2\n",
    "        TOMATO_IDX = 3\n",
    "\n",
    "\n",
    "        POT_EMPTY_IDX = 0\n",
    "        POT_FULL_IDX = 1\n",
    "        POT_COOKING_IDX = 2\n",
    "        POT_READY_IDX = 3\n",
    "\n",
    "\n",
    "        prev_holding_vector = prev_obs[4:8]\n",
    "        current_holding_vector = current_obs[4:8]\n",
    "\n",
    "        # Was not holding anything, now holding an ingredient or empty dish\n",
    "        if prev_holding_vector.sum() == 0 and current_holding_vector.sum() == 1:\n",
    "            if current_holding_vector[ONION_IDX] == 1 or current_holding_vector[TOMATO_IDX] == 1:\n",
    "                shaping += 0.05 # Reward for picking up an ingredient\n",
    "            elif current_holding_vector[DISH_IDX] == 1:\n",
    "                shaping += 0.02 # Reward for picking up an empty dish\n",
    "\n",
    "        prev_pot_onions = prev_obs[27:28][0] # Assuming single value\n",
    "        current_pot_onions = current_obs[27:28][0]\n",
    "        prev_pot_tomatoes = prev_obs[28:29][0]\n",
    "        current_pot_tomatoes = current_obs[28:29][0]\n",
    "\n",
    "        # Check if ingredient count in closest pot increased\n",
    "        if current_pot_onions > prev_pot_onions:\n",
    "            shaping += 0.1 # Reward for adding onion\n",
    "        if current_pot_tomatoes > prev_pot_tomatoes:\n",
    "            shaping += 0.1 # Reward for adding tomato\n",
    "\n",
    "\n",
    "\n",
    "        prev_pot_states = prev_obs[23:27]\n",
    "        current_pot_states = current_obs[23:27]\n",
    "\n",
    "        # Transition from full to cooking\n",
    "        if prev_pot_states[POT_FULL_IDX] == 1 and current_pot_states[POT_COOKING_IDX] == 1:\n",
    "            shaping += 0.2 # Reward for starting to cook\n",
    "\n",
    "        # Transition from cooking to ready\n",
    "        if prev_pot_states[POT_COOKING_IDX] == 1 and current_pot_states[POT_READY_IDX] == 1:\n",
    "            shaping += 0.3 # Reward for soup becoming ready\n",
    "\n",
    "\n",
    "        if prev_pot_states[POT_READY_IDX] == 1 and current_holding_vector[SOUP_IDX] == 1 and prev_holding_vector[SOUP_IDX] == 0:\n",
    "            shaping += 0.25 # Reward for picking up a ready soup\n",
    "\n",
    "\n",
    "        current_dx_serving = abs(current_obs[16:17][0]) # abs(dx)\n",
    "        current_dy_serving = abs(current_obs[17:18][0]) # abs(dy)\n",
    "        prev_dx_serving = abs(prev_obs[16:17][0])\n",
    "        prev_dy_serving = abs(prev_obs[17:18][0])\n",
    "\n",
    "        current_dist_serving = current_dx_serving + current_dy_serving # Manhattan distance\n",
    "        prev_dist_serving = prev_dx_serving + prev_dy_serving\n",
    "\n",
    "        if current_holding_vector[SOUP_IDX] == 1 and current_dist_serving < prev_dist_serving:\n",
    "            shaping += 0.01 # Small continuous reward for moving towards serving\n",
    "\n",
    "\n",
    "\n",
    "        return shaping"
   ],
   "id": "3cd2d20dfbbb0e54",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T19:13:38.266507Z",
     "start_time": "2025-07-22T19:13:38.220509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pygame\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_size,input_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=input_size, out_features=64)\n",
    "        self.dense2 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dense3 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dense4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.policy_logits = nn.Linear(in_features=64, out_features=action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.dense1(state))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = F.relu(self.dense3(x))\n",
    "        x = F.relu(self.dense4(x))\n",
    "        x = F.relu(self.dense5(x))\n",
    "        logits = self.policy_logits(x)\n",
    "        return logits\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, action_size,input_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=input_size, out_features=64) # You'll need to specify input features here\n",
    "        self.dense2 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dense3 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dense4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.value = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.dense1(state))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = F.relu(self.dense3(x))\n",
    "        x = F.relu(self.dense4(x))\n",
    "        x = F.relu(self.dense5(x))\n",
    "        value = self.value(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OvercookedPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layout_name,\n",
    "        model_actor,\n",
    "        model_critic,\n",
    "        gamma,  # Discount factor\n",
    "        lr_actor,  # Actor learning rate\n",
    "        lr_critic,  # Critic learning rate\n",
    "        clip_ratio, # PPO clip ratio\n",
    "        epochs, # Number of optimization epochs\n",
    "        batch_size,\n",
    "        optimizer_class,\n",
    "        lmbda,\n",
    "        entropy_coefficient\n",
    "\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_coefficient = entropy_coefficient\n",
    "\n",
    "        self.individual_action_values = Action.ALL_ACTIONS\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "\n",
    "\n",
    "        base_mdp = OvercookedGridworld.from_layout_name(layout_name) # or other layout\n",
    "        base_env = OvercookedEnv.from_mdp(base_mdp, info_level=0, horizon=400)\n",
    "        env = Overcooked(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "        self.env = env\n",
    "        #self.env  = OvercookedRewardShaping(base_env=base_env, featurize_fn=base_env.featurize_state_mdp)\n",
    "\n",
    "        self.n_possible_action = self.env.action_space.n\n",
    "\n",
    "        dummy_state = self.env.reset()\n",
    "        dummy_obs_agent0 = dummy_state['both_agent_obs'][0]\n",
    "        state_input_size = len(dummy_obs_agent0) # Assuming the featurized state is a flat vector\n",
    "\n",
    "        self.actor_model = model_actor(self.n_possible_action, state_input_size).to(self.device)\n",
    "        self.critic_model = model_critic(self.n_possible_action, state_input_size).to(self.device)\n",
    "        self.actor_optimizer = optimizer_class(self.actor_model.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optimizer_class(self.critic_model.parameters(), lr=lr_critic)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def ppo_loss(self,old_logits, old_values, returns, states, actions,dones):\n",
    "\n",
    "        old_logits0_initial, old_logits1_initial = old_logits\n",
    "        old_values0_initial, old_values1_initial = old_values\n",
    "\n",
    "        def get_advantages_gae(values_tensor, masks_tensor, rewards_tensor):\n",
    "            # values_tensor should be (batch_size, 1), squeeze it\n",
    "            values_squeezed = values_tensor.squeeze(-1) # shape (batch_size,)\n",
    "            advantages_tensor = torch.zeros_like(rewards_tensor).to(self.device)\n",
    "            last_gae_lam = 0\n",
    "            for t in reversed(range(len(rewards_tensor))):\n",
    "                if t == len(rewards_tensor) - 1: # Last step in the collected trajectory\n",
    "\n",
    "                    next_value = 0.0 # If episode is truly done, next value is 0\n",
    "                else:\n",
    "                    next_value = values_squeezed[t+1] # V(s_{t+1})\n",
    "\n",
    "                delta = rewards_tensor[t] + self.gamma * next_value * masks_tensor[t] - values_squeezed[t]\n",
    "                last_gae_lam = delta + self.gamma * self.lmbda * masks_tensor[t] * last_gae_lam\n",
    "                advantages_tensor[t] = last_gae_lam\n",
    "\n",
    "            returns_gae = advantages_tensor + values_squeezed\n",
    "\n",
    "            # Normalize advantages\n",
    "            advantages_normalized = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-10)\n",
    "\n",
    "            return returns_gae, advantages_normalized\n",
    "\n",
    "\n",
    "        def compute_loss(logits, values, actions, returns,old_logits,advantages):\n",
    "            actions_onehot = F.one_hot(actions.long(), num_classes=self.n_possible_action).float()\n",
    "\n",
    "            policy = F.softmax(logits, dim=-1)\n",
    "            action_probs = torch.sum(actions_onehot * policy, dim=-1) # Use dim=-1 for last dimension\n",
    "\n",
    "            old_policy = F.softmax(old_logits.detach(), dim=-1)\n",
    "            old_action_probs = torch.sum(actions_onehot * old_policy, dim=-1)\n",
    "\n",
    "            epsilon = 1e-10\n",
    "            # Policy loss\n",
    "\n",
    "\n",
    "            ratio = torch.exp(torch.log(action_probs + 1e-10) - torch.log(old_action_probs + 1e-10))\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "            policy_loss = -torch.mean(torch.min(ratio * advantages, clipped_ratio * advantages))\n",
    "\n",
    "\n",
    "            # Value loss\n",
    "            returns_tensor = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "            value_loss = torch.mean(torch.square(values.squeeze(-1) - returns_tensor)) # Squeeze value to match returns shape\n",
    "\n",
    "            # Entropy bonus (optional)\n",
    "            # Ensure policy is not zero for log\n",
    "            entropy_bonus = torch.mean(policy * torch.log(policy + epsilon)) # PyTorch's entropy loss is typically negative\n",
    "\n",
    "            total_loss = policy_loss + self.entropy_coefficient * entropy_bonus # Note: In PyTorch, entropy_bonus typically added for maximization, so + sign. If you want regularization that penalizes low entropy, it's typically subtracted like in TF. Let's keep your original -0.01 for regularization.\n",
    "            return total_loss, value_loss\n",
    "\n",
    "        def train_step(states, actions, returns, old_logits, old_values, advantages_tuple):\n",
    "            self.actor_optimizer.zero_grad() # Zero gradients for this optimization step\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            logits0 = self.actor_model(states[0])\n",
    "            values0 = self.critic_model(states[0])\n",
    "            logits1 = self.actor_model(states[1])\n",
    "            values1 = self.critic_model(states[1])\n",
    "\n",
    "            policy_loss0, value_loss0 = compute_loss(logits0, values0, actions[0], returns, old_logits[0], advantages_tuple[0])\n",
    "\n",
    "            policy_loss1, value_loss1 = compute_loss(logits1, values1, actions[1], returns, old_logits[1], advantages_tuple[1])\n",
    "\n",
    "            total_value_loss = value_loss0 + value_loss1\n",
    "            total_policy_loss = policy_loss0 + policy_loss1\n",
    "\n",
    "            total_value_loss.backward()\n",
    "            total_policy_loss.backward() # Compute gradients\n",
    "\n",
    "            self.actor_optimizer.step() # Zero gradients for this optimization step\n",
    "            self.critic_optimizer.step() # Update model parameters\n",
    "\n",
    "            return total_value_loss.item(), total_policy_loss.item() # Return scalar loss values\n",
    "\n",
    "        current_loss0, current_loss1 = 0, 0\n",
    "        rewards_tensor = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "        masks_tensor = torch.tensor(dones, dtype=torch.float32).to(self.device) # `dones_list` is `not done` (1 for non-terminal, 0 for terminal)\n",
    "        returns0_gae, advantages0 = get_advantages_gae(old_values0_initial, masks_tensor, rewards_tensor)\n",
    "        returns1_gae, advantages1 = get_advantages_gae(old_values1_initial, masks_tensor, rewards_tensor)\n",
    "        for _ in range(self.epochs):\n",
    "            # Pass all initial data, the inner train_step will recompute current model outputs\n",
    "            current_loss0, current_loss1 = train_step(\n",
    "                (states[0], states[1]),\n",
    "                (actions[0], actions[1]),\n",
    "                returns,\n",
    "                (old_logits0_initial, old_logits1_initial),\n",
    "                (old_values0_initial, old_values1_initial),\n",
    "                (advantages0, advantages1)\n",
    "            )\n",
    "\n",
    "        return current_loss0, current_loss1\n",
    "\n",
    "\n",
    "    def trainingLoop(self,max_episodes,max_steps_per_episode):\n",
    "        for episode in range(max_episodes):\n",
    "            states0,states1, actions0, actions1, rewards, values1,values0, returns,dones = [], [], [], [], [],[],[],[],[]\n",
    "            state = self.env.reset()\n",
    "            for step in range(max_steps_per_episode):\n",
    "                state0_tensor = torch.tensor(state['both_agent_obs'][0], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                state1_tensor = torch.tensor(state['both_agent_obs'][1], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "                self.actor_model.eval()\n",
    "                self.critic_model.eval()\n",
    "                with torch.no_grad(): # No need to track gradients for action selection\n",
    "                    logits0 = self.actor_model(state0_tensor)\n",
    "                    value0 = self.critic_model(state0_tensor)\n",
    "                    logits1 = self.actor_model(state1_tensor)\n",
    "                    value1 = self.critic_model(state1_tensor)\n",
    "                self.actor_model.train()\n",
    "                self.critic_model.train() # Sw\n",
    "\n",
    "                action_dist0 = torch.distributions.Categorical(logits=logits0)\n",
    "                action0 = action_dist0.sample().item() # Get scalar action\n",
    "\n",
    "                action_dist1 = torch.distributions.Categorical(logits=logits1)\n",
    "                action1 = action_dist1.sample().item()\n",
    "\n",
    "                action = (action0, action1)\n",
    "                next_state, reward, done, event_info = self.env.step(action)\n",
    "\n",
    "                states0.append(state0_tensor)\n",
    "                states1.append(state1_tensor)\n",
    "                actions0.append(action0)\n",
    "                actions1.append(action1)\n",
    "                rewards.append(reward)\n",
    "                values0.append(value0)\n",
    "                values1.append(value1)\n",
    "                dones.append(not done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "\n",
    "\n",
    "                    # Convert lists of tensors/numpy arrays to batched tensors\n",
    "                    states0_batch = torch.cat(states0, dim=0).to(self.device)\n",
    "                    states1_batch = torch.cat(states1, dim=0).to(self.device)\n",
    "\n",
    "                    actions0_batch = torch.tensor(actions0, dtype=torch.float32).to(self.device) # Actions should be long for indexing/one-hot\n",
    "                    actions1_batch = torch.tensor(actions1, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    # Values are already tensors from model output, concatenate them\n",
    "                    values0_batch = torch.cat(values0, dim=0).to(self.device)\n",
    "                    values1_batch = torch.cat(values1, dim=0).to(self.device)\n",
    "\n",
    "                    old_logits0_batch = self.actor_model(states0_batch)\n",
    "                    old_logits1_batch = self.actor_model(states1_batch)\n",
    "\n",
    "                    total_value_loss, total_policy_loss = self.ppo_loss(\n",
    "                        (old_logits0_batch, old_logits1_batch),\n",
    "                        (values0_batch, values1_batch),\n",
    "                        rewards,\n",
    "                        (states0_batch, states1_batch),\n",
    "                        (actions0_batch, actions1_batch),\n",
    "                        dones\n",
    "                    )\n",
    "                    if episode % 10 == 0:\n",
    "                        print(f\"Episode: {episode + 1}, value_loss : {total_value_loss}, policy_loss : {total_policy_loss}, total reward {np.array(rewards).sum()}\")\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "    def test(self, n_episodes, visualize=False, print_action=False):\n",
    "        pygame.init()\n",
    "        visualizer = StateVisualizer()\n",
    "\n",
    "        # 2) Grab your grid and do one dummy render to get a surface\n",
    "        grid = self.env.base_env.mdp.terrain_mtx\n",
    "        _ = self.env.reset()\n",
    "        surf = visualizer.render_state(self.env.base_env.state, grid=grid)\n",
    "\n",
    "        # 3) Use that surface’s size for your window\n",
    "        win_w, win_h = surf.get_size()\n",
    "        screen = pygame.display.set_mode((win_w, win_h), pygame.RESIZABLE)\n",
    "        clock  = pygame.time.Clock()\n",
    "\n",
    "        running = True\n",
    "        obs = self.env.reset() #observation of the starting state\n",
    "        soup_delivered = 0\n",
    "\n",
    "        total_rewards = []\n",
    "        while running:\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            self.actor_model.eval()\n",
    "            self.critic_model.eval() # Set model to evaluation mode\n",
    "            with torch.no_grad(): # No gradient calculation during testing\n",
    "                state0_tensor = torch.tensor(obs['both_agent_obs'][0], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                state1_tensor = torch.tensor(obs['both_agent_obs'][1], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "                logits0 = self.actor_model(state0_tensor)\n",
    "                logits1 = self.actor_model(state1_tensor)\n",
    "\n",
    "                action0 = torch.argmax(logits0, dim=1).item()\n",
    "                action1 = torch.argmax(logits1, dim=1).item()\n",
    "\n",
    "                if print_action:\n",
    "                    print(action0, action1)\n",
    "                # try to step; if episode is over, catch and reset\n",
    "                try:\n",
    "                    # Overcooked wrapper returns (obs_p0, obs_p1, reward, done, info)\n",
    "                    obs, reward, done, info = self.env.step((action0, action1))\n",
    "                    if reward > 19:\n",
    "                        soup_delivered += 1\n",
    "                except AssertionError:\n",
    "                    # base_env.is_done() was True → reset and continue\n",
    "                    self.env.reset()\n",
    "                    break\n",
    "\n",
    "                # render the new state\n",
    "                surf = visualizer.render_state(self.env.base_env.state, grid=grid)\n",
    "\n",
    "                # draw it\n",
    "                screen.blit(surf, (0, 0))\n",
    "                pygame.display.flip()\n",
    "\n",
    "                clock.tick(15)   # cap at 30 FPS\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "        print(f\"Soup delivered: {soup_delivered}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "240394b8b3aec7f1",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T19:13:38.960756Z",
     "start_time": "2025-07-22T19:13:38.935724Z"
    }
   },
   "cell_type": "code",
   "source": "OPPO = OvercookedPPO(\"cramped_room\",model_actor =Actor,model_critic=Critic,gamma = 0.99,lr_actor = 0.001,lr_critic = 0.0005,clip_ratio = 0.3 ,epochs = 10,batch_size = 128,optimizer_class = torch.optim.Adam,lmbda=0.95,entropy_coefficient=0.25)",
   "id": "978e960c12810ac8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T19:13:02.577420Z",
     "start_time": "2025-07-22T19:11:41.279356Z"
    }
   },
   "cell_type": "code",
   "source": "OPPO.trainingLoop(max_episodes = 500,max_steps_per_episode = 2000)",
   "id": "f16eee44ecc0b7d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, value_loss : 3.853119051200338e-05, policy_loss : -0.17766714096069336, total reward 0\n",
      "Episode: 11, value_loss : 3.271579771535471e-07, policy_loss : -0.19052007794380188, total reward 0\n",
      "Episode: 21, value_loss : 9.983022408732722e-08, policy_loss : -0.18197530508041382, total reward 0\n",
      "Episode: 31, value_loss : 1.5486786253404716e-07, policy_loss : -0.17208828032016754, total reward 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mOPPO\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainingLoop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_episodes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmax_steps_per_episode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[49], line 208\u001B[0m, in \u001B[0;36mOvercookedPPO.trainingLoop\u001B[1;34m(self, max_episodes, max_steps_per_episode)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_steps_per_episode):\n\u001B[0;32m    207\u001B[0m     state0_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth_agent_obs\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 208\u001B[0m     state1_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mboth_agent_obs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    210\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor_model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcritic_model\u001B[38;5;241m.\u001B[39meval()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T19:14:20.370335Z",
     "start_time": "2025-07-22T19:14:16.878328Z"
    }
   },
   "cell_type": "code",
   "source": "OPPO.test(n_episodes = 100,visualize = True,print_action = True)",
   "id": "2dcd2d4e41b17c76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "Soup delivered: 0\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f353440204a4a2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
